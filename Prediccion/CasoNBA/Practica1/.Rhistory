knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(leaps)
library(ggplot2)
library(ISLR)
library(glmnet)
library(caret)
library(rsample)
library(dplyr)
mData <- read.csv("nba.csv")
names(mData)
str(mData)
1 - (sum(complete.cases(mData)) / nrow(mData))
mData <- na.omit(mData)
mData <- select(mData, -Player, -NBA_Country)
mejores_modelos <- regsubsets(Salary~., data = mData, nvmax = 25, really.big = TRUE, method = "forward")
summary(mejores_modelos)
summary(mejores_modelos)$bic
which.min(summary(mejores_modelos)$bic)
names(summary(mejores_modelos))
library(tidyverse)
library(leaps)
library(ggplot2)
library(ISLR)
library(glmnet)
library(caret)
library(rsample)
library(dplyr)
mData <- read.csv("nba.csv")
1 - (sum(complete.cases(mData)) / nrow(mData))
mData <- na.omit(mData)
mData <- select(mData, -Player, -NBA_Country)
set.seed(1)
training <- sample(x = 1:483, size = 322, replace = FALSE)
set.seed(11)
grupo <- sample(rep(x = 1:20, length = nrow(mData)))
#Se comprueba que la distribución es aproximadamente equitativa
table(grupo)
predict.regsubsets <- function(object, newdata, id, ...){
# Extraer la fórmula del modelo (variable dependiente ~ predictores)
form <- as.formula(object$call[[2]])
# Generar una matriz modelo con los nuevos datos y la fórmula
mat <- model.matrix(form, newdata)
# Extraer los coeficientes del modelo
coefi <- coef(object , id = id)
# Almacenar el nombre de las variables predictoras del modelo
xvars <- names(coefi)
# Producto matricial entre los coeficientes del modelo y los valores de
# los predictores de las nuevas observaciones para obtener las
# predicciones
mat[ , xvars] %*% coefi
}
names(summary(predict.regsubsets()))
summary(predict.regsubsets())
View(predict.regsubsets)
error_matrix <- matrix(data = NA, nrow = 20, ncol = 25,
dimnames = list(NULL, c(1:25)))
# Loop en el que se excluye en cada iteración un grupo distinto
# ESTE LOOP ESTA HECHO PARA UN DATA FRAME CON 19 PREDICTORES
num_validaciones <- 20
num_predictores <- 25
for (k in 1:num_validaciones) {
# Identificación de datos empleados como training
training <- mData[grupo != k, ]
# Selección de los mejores modelos para cada tamaño basándose en RSS
mejores_modelos <- regsubsets(Salary~., data = training, nvmax = 25,
method = "forward")
# Para cada uno de los modelos "finalistas" se calcula el test-error con
# el grupo excluido
for (i in 1:num_predictores) {
test <- mData[grupo == k, ]
# Las predicciones del modelo i almacenado en el objeto regsubsets se
# extraen mediante la función predict.regsubsets() definida arriba
predicciones <- predict.regsubsets(object = mejores_modelos,
newdata = test, id = i)
# Cálculo y almacenamiento del MSE para el modelo i
error_matrix[k,i] <- mean((test$Salary - predicciones)^2)
}
}
View(error_matrix)
mean_cv_error <- apply(X = error_matrix, MARGIN = 2, FUN = mean)
which.min(x = mean_cv_error)
ggplot(data = data.frame(n_predictores = 1:25, mean_cv_error = mean_cv_error),
aes(x = n_predictores, y = mean_cv_error)) +
geom_line() +
geom_point() +
geom_point(aes(x = n_predictores[which.min(mean_cv_error)],
y = mean_cv_error[which.min(mean_cv_error)]),
colour = "red", size = 3) +
scale_x_continuous(breaks = c(0:25)) +
theme_bw() +
labs(title = "Cross-validation mean error vs número de predictores",
x = "número predictores")
