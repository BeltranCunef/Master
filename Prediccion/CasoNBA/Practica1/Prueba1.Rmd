---
title: "Prueba1"
author: "Beltran"
date: "8/10/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Cargo las librerías necesarias en un primer momento
```{r}
library(tidyverse)
library(dplyr)
```
#Lectura del fichero nba.csv
```{r}
mData=read.csv("nba.csv")
```
Observo la información acerca de las variables contenidas en el dataset
```{r}
summary(mData)
```
Elimino los valores NA
```{r}
mData <- na.omit(mData)
```
#Establecimiento de la regresión
```{r}
regres01=lm(Salary~NBA_DraftNumber+log(Age)+Tm+G+log(MP)+PER+TS.+TRB.+AST.+STL.+BLK.+TOV.+USG.+WS
            +BPM+VORP, data = mData)
summary(regres01)
```
En la regresion asumo que la edad y los minutos jugados por ejemplo siguen una funcion logarítmica,
llegado un momento, tener más años o jugar más minutos no contribuye a un mayor salario. La variable de conversión de tiro incluye datos acerca de la conversión de tiros de 2, de 3 y tiros libres. Por tanto,
esta ya incluye información acerca de las 3 ya mencionadas. Algo parecido sucede con el porcentaje de 
rebotes ganados, este variable contiene los rebotes ofensivos y defensivos a su vez.
#Normalidad
##qqplot
```{r}
library(car)
qqPlot(regres01, labels=row.names(mData), id.method="identify",
       simulate=TRUE, main="Q-Q Plot")
```
Comprobamos si la distribución de la muestra se asemeja a una normal.
##Histograma+densidad+normal+rug
```{r}
residplot <- function(fit, nbreaks=10) {
  z <- rstudent(fit)
  hist(z, breaks=nbreaks, freq=FALSE,
       xlab="Studentized Residual",
       main="Distribution of Errors")
  rug(jitter(z), col="brown")
  curve(dnorm(x, mean=mean(z), sd=sd(z)),
        add=TRUE, col="blue", lwd=2)
  lines(density(z)$x, density(z)$y,
        col="red", lwd=2, lty=2)
  legend("topright",
         legend = c( "Normal Curve", "Kernel Density Curve"),
         lty=1:2, col=c("blue","red"), cex=.7)
}
residplot(regres01)
```
Represento gráficamente la distribución de los errores.
Para comprobar la normalidad de la distribución realizaré los contrastes de Jaque-Bera y Shapiro-Wilk.
#Jarque Bera
```{r}
vResid=resid(regres01)
library(fBasics)
```
```{r}
jbTest(vResid)
```
Dada la muestra y el p-value obtenido, con un nivel de significancia del 5% se procede a rechazar la hipótesis nula, por tanto se asume la no normalidad de la muestra.
#Shapiro-Wilk
```{r}
shapiro.test(vResid)
```
De nuevo, con los datos de la muestra y el p-value obtenido, a un nivel de significancia del 5% se procede a rechazar la hipótesis nula y asumir la no normalidad de la distribución.
##Homocedasticidad
Llevo a cabo el contraste de Breusch-Pagan para combrobar si el modelo es homocedástico o heterocedástico.
```{r}
ncvTest(regres01)
```
Con los datos de la muestra y el p-valor obtenido, para un nivel de significatividad del 5% se rechaza la hipótesis nula, el modelo es heterocedástico.
```{r}
spreadLevelPlot(regres01)
```
##Validación global
Cabe la posibilidad de llevar a cabo todos los contrastes de hipótesis a la vez, mediante el test de Peña.
```{r}
library(gvlma)
gvmodel <- gvlma(regres01) 
summary(gvmodel)
```
##Multicolinealidad
```{r}
vif(regres01) 
```
Para valores de la raíz superiores a 2 se detecta un problema de multicolinealidad en las variables, se deben retirar estas del modelo una a una y repetir la prueba de multicolinealidad.

Elimino BPM en primer lugar, establezco la nueva regresión y compruebo la multicolinealidad de nuevo.
```{r}
regres01=lm(Salary~NBA_DraftNumber+log(Age)+Tm+G+log(MP)+PER+TS.+TRB.+AST.+STL.+BLK.+TOV.+USG.+WS
            +VORP, data = mData)
summary(regres01)
```
```{r}
vif(regres01) 
```
A continuación elimino WS.
```{r}
regres01=lm(Salary~NBA_DraftNumber+log(Age)+Tm+G+log(MP)+PER+TS.+TRB.+AST.+STL.+BLK.+TOV.+USG.
            +VORP, data = mData)
summary(regres01)
```
```{r}
vif(regres01)
```
Ahora elimino log(MP).
```{r}
regres01=lm(Salary~NBA_DraftNumber+log(Age)+Tm+G+PER+TS.+TRB.+AST.+STL.+BLK.+TOV.+USG.
            +VORP, data = mData)
summary(regres01)
```
```{r}
vif(regres01)
```
Procedo a eliminar PER.
```{r}
regres01=lm(Salary~NBA_DraftNumber+log(Age)+Tm+G+TS.+TRB.+AST.+STL.+BLK.+TOV.+USG.
            +VORP, data = mData)
summary(regres01)
```
```{r}
vif(regres01)
```
Consigo eliminar la multicolinealidad del modelo. Como contraprestación, el R-squared del modelo disminuye.
##Observaciones anómalas
```{r}
outlierTest(regres01)
```
Represento los valores extremos.
```{r}
hat.plot <- function(fit) {
  p <- length(coefficients(fit))
  n <- length(fitted(fit))
  plot(hatvalues(fit), main="Index Plot of Hat Values")
  abline(h=c(2,3)*p/n, col="red", lty=2)
  identify(1:n, hatvalues(fit), names(hatvalues(fit)))
}
hat.plot(regres01)
```
Llevo a cabo el cálculo de la distancia de Cook.
```{r}
cutoff <- 4/(nrow(mData)-length(regres01$coefficients)-2)
plot(regres01, which=4, cook.levels=cutoff)
abline(h=cutoff, lty=2, col="red")
```
```{r}
influencePlot(regres01, id.method="identify", main="Influence Plot", 
              sub="Circle size is proportial to Cook's Distance" )
```
#Selección de variables
Uso el método Forward Stepwise
```{r}
library(MASS)
library(leaps)
regfit.fwd=regsubsets(Salary~NBA_DraftNumber+log(Age)+Tm+G+TS.+TRB.+AST.+STL.+BLK.+TOV.+USG.+VORP,mData,method ="forward")
summary (regfit.fwd )
```
```{r}
stepAIC(regres01, direction="both")
```
Después de los cálculos llevados a cabo nos quedamos con una nueva regresión de 4 variables solamente. NBA-DraftNumber, log(Age), USG. y VORP.
Por tanto el mejor modelo es:
```{r}
regres02=lm(Salary~NBA_DraftNumber+log(Age)+USG.+VORP, data = mData)
summary(regres02)
```
#Cross Validation
##Validation Test
```{r}
library(ISLR)
set.seed(250)
numData=nrow(mData)
train=sample(numData ,numData/2)

regres.train =lm(Salary~NBA_DraftNumber + log(Age) + USG. + VORP,mData ,subset =train )
attach(mData)
mean((Salary-predict(regres.train ,Auto))[-train ]^2)
```
```{r}
glm.fit1=glm(Salary~NBA_DraftNumber + log(Age) + USG. + VORP,mData,family = gaussian())
coef(glm.fit1)
```
```{r}
library(boot)
```
```{r}
cv.err =cv.glm(mData,glm.fit1)
cv.err$delta
```
